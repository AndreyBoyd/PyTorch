{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fe739db",
   "metadata": {},
   "source": [
    "### Домашняя работа к уроку 5\n",
    "\n",
    "Обучить GRU, LSTM для предсказания временного ряда на примере https://www.kaggle.com/c/favorita-grocery-sales-forecasting (для каждого типа продуктов)\n",
    "\n",
    "Примечание: поскольку датасет с kaggle.com загрузить не удалось (требует верификацию телефона, которую не удаётся пройти) делаю с данными из урока просто чтобы посмотреть как работают сети GRU, LSTM на практике."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f7c0117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import re\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm import notebook\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from functools import lru_cache\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14307529",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data3/train.csv\")\n",
    "df_test = pd.read_csv(\"data3/test.csv\")\n",
    "df_val = pd.read_csv(\"data3/val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8811abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(get_stop_words(\"ru\"))\n",
    "exclude = set(punctuation)\n",
    "morpher = MorphAnalyzer()\n",
    "\n",
    "def preprocess_text(txt):\n",
    "    txt = str(txt)\n",
    "    txt = \"\".join(c for c in txt if c not in exclude)\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(\"не\\s*\", \"не\", txt)\n",
    "    txt = [morpher.parse(word)[0].normal_form for word in txt.split() if word not in sw]\n",
    "    return \" \".join(txt)\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(preprocess_text)\n",
    "df_val['text'] = df_val['text'].apply(preprocess_text)\n",
    "df_test['text'] = df_test['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3753c585",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus_train = df_train['text'].values\n",
    "text_corpus_valid = df_val['text'].values\n",
    "text_corpus_test = df_test['text'].values\n",
    "\n",
    "counts = Counter()\n",
    "for sequence in text_corpus_train:\n",
    "    counts.update(sequence.split())\n",
    "\n",
    "for word in list(counts):\n",
    "    if counts[word] < 2:\n",
    "        del counts[word]\n",
    "    \n",
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af605091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, txts, labels, w2index, used_length):\n",
    "        self._txts = txts\n",
    "        self._labels = labels\n",
    "        self._length = used_length\n",
    "        self._w2index = w2index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._txts)\n",
    "    \n",
    "    @lru_cache(50000)\n",
    "    def encode_sentence(self, txt):\n",
    "        encoded = np.zeros(self._length, dtype=int)\n",
    "        enc1 = np.array([self._w2index.get(word, self._w2index[\"UNK\"]) for word in txt.split()])\n",
    "        length = min(self._length, len(enc1))\n",
    "        encoded[:length] = enc1[:length]\n",
    "        return encoded, length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        encoded, length = self.encode_sentence(self._txts[index])\n",
    "        return torch.from_numpy(encoded.astype(np.int32)), self._labels[index], length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "866e2d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['class'].values\n",
    "y_val = df_val['class'].values\n",
    "\n",
    "train_dataset = TwitterDataset(text_corpus_train, y_train, vocab2index, 27)\n",
    "valid_dataset = TwitterDataset(text_corpus_valid, y_val, vocab2index, 27)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                          batch_size=128,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                          batch_size=128,\n",
    "                          shuffle=False,\n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b3f46e",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d8310d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMFixedLen(nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(lstm_out)\n",
    "    \n",
    "lstm_init = LSTMFixedLen(len(vocab2index), 30, 20)\n",
    "optimizer = torch.optim.Adam(lstm_init.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a00180b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f89f614b47418d89a15f5c8022d032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 valid_loss 113.31182861328125\n",
      "Epoch 1 valid_loss 113.09640502929688\n",
      "Epoch 2 valid_loss 118.00149536132812\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "for epoch in notebook.tqdm(range(3)):  \n",
    "    lstm_init.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels, lengths = data[0], data[1], data[2]\n",
    "        inputs = inputs.long()\n",
    "        labels = labels.long().view(-1, 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = lstm_init(inputs, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    lstm_init.eval()\n",
    "    loss_accumed = 0\n",
    "    for X, y, lengths in valid_loader:\n",
    "        X = X.long()\n",
    "        y = y.long().view(-1, 1)\n",
    "        output = lstm_init(X, lengths)\n",
    "        loss = criterion(output, y)\n",
    "        loss_accumed += loss\n",
    "    print(\"Epoch {} valid_loss {}\".format(epoch, loss_accumed))\n",
    "\n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e27dee",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9108cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUFixedLen(nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.gru(x)\n",
    "        return self.linear(lstm_out)\n",
    "    \n",
    "gru_init = GRUFixedLen(len(vocab2index), 30, 20)\n",
    "optimizer = torch.optim.Adam(gru_init.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71ddacd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a3a0fd07b14e4d935a531791fa1d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 valid_loss 113.16790771484375\n",
      "Epoch 1 valid_loss 112.73062896728516\n",
      "Epoch 2 valid_loss 114.7308578491211\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "for epoch in notebook.tqdm(range(3)):  \n",
    "    gru_init.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels, lengths = data[0], data[1], data[2]\n",
    "        inputs = inputs.long()\n",
    "        labels = labels.long().view(-1, 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = gru_init(inputs, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    gru_init.eval()\n",
    "    loss_accumed = 0\n",
    "    for X, y, lengths in valid_loader:\n",
    "        X = X.long()\n",
    "        y = y.long().view(-1, 1)\n",
    "        output = gru_init(X, lengths)\n",
    "        loss = criterion(output, y)\n",
    "        loss_accumed += loss\n",
    "    print(\"Epoch {} valid_loss {}\".format(epoch, loss_accumed))\n",
    "\n",
    "print('Training is finished!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
