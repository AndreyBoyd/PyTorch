{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ea2834",
   "metadata": {},
   "source": [
    "### Домашнее задание\n",
    "\n",
    "Переписать загрузку данных с python функций на Dataset и Dataloader и применить сеть с attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c22bf83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d249d09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 30\n",
    "latent_dim = 256\n",
    "num_samples = 10000\n",
    "data_path = './data6/fra-eng/fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9824baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем из текстов токены\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "input_vocab = set()\n",
    "output_vocab = set()\n",
    "\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    for word in input_text.split():\n",
    "        input_vocab.add(word.strip())\n",
    "    target_texts.append(target_text)\n",
    "    for word in target_text.split():\n",
    "        output_vocab.add(word.strip())\n",
    "    \n",
    "input_vocab2index = {word: i+2 for i, word in enumerate(input_vocab)}\n",
    "output_vocab2index = {word: i+2 for i, word in enumerate(output_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "611e7690",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab = list(input_vocab)\n",
    "output_vocab = list(output_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2783772e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3111\n",
      "approves.\n"
     ]
    }
   ],
   "source": [
    "class Input(Dataset):\n",
    "    def __init__(self):\n",
    "        self.input_vocab = input_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_vocab[idx]\n",
    "    \n",
    "    def indexesFromSentence(sentence, vocab):\n",
    "        return [vocab.get(word.strip(), 0) for word in sentence.split(' ')]\n",
    "\n",
    "    def tensorFromSentence(sentence, vocab):\n",
    "        indexes = indexesFromSentence(sentence, vocab)\n",
    "        indexes.append(1)\n",
    "        return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "    def tensorsFromSent(input_sentences):\n",
    "        input_tensor = tensorFromSentence(input_sentences, input_vocab2index)\n",
    "        return (input_tensor)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = Input()\n",
    "    print(len(dataset))\n",
    "    print(dataset[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97050b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5757\n",
      "amuse-toi.\n"
     ]
    }
   ],
   "source": [
    "class Output(Dataset):\n",
    "    def __init__(self):\n",
    "        self.output_vocab = output_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.output_vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.output_vocab[idx]\n",
    "    \n",
    "    def indexesFromSentence(sentence, vocab):\n",
    "        return [vocab.get(word.strip(), 0) for word in sentence.split(' ')]\n",
    "\n",
    "    def tensorFromSentence(sentence, vocab):\n",
    "        indexes = indexesFromSentence(sentence, vocab)\n",
    "        indexes.append(1)\n",
    "        return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "    def tensorsFromSent(output_sentences):\n",
    "        target_tensor = tensorFromSentence(input_sentences, output_vocab2index)\n",
    "        return (target_tensor)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = Output()\n",
    "    print(len(dataset))\n",
    "    print(dataset[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d8ddf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = Input(), Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ff5d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(df_train,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=0)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(df_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc3a766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(sentence, vocab):\n",
    "    return [vocab.get(word.strip(), 0) for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(sentence, vocab):\n",
    "    indexes = indexesFromSentence(sentence, vocab)\n",
    "    indexes.append(1)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromSent(input_sentences, output_sentences):\n",
    "    input_tensor = tensorFromSentence(input_sentences, input_vocab2index)\n",
    "    target_tensor = tensorFromSentence(output_sentences, output_vocab2index)\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e55d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input_, hidden):\n",
    "        embedded = self.embedding(input_).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd397c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_).view(1, 1, -1)\n",
    "        output, hidden = self.lstm(embedded, hidden.view(1, 1, 1, -1))\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19203997",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=10):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        #output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8877da01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=10):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[0]])\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == 1:\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "550aedaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0 0%) 8.5609\n",
      "(1 10%) 8.7300\n",
      "(2 20%) 8.6807\n",
      "(3 30%) 8.6533\n",
      "(4 40%) 8.7391\n",
      "(5 50%) 8.4671\n",
      "(6 60%) 8.6613\n",
      "(7 70%) 8.5622\n",
      "(8 80%) 8.5606\n",
      "(9 90%) 8.5641\n",
      "(10 100%) 8.6097\n",
      "(11 110%) 8.6060\n",
      "(12 120%) 8.6144\n",
      "(13 130%) 8.5854\n",
      "(14 140%) 8.5922\n",
      "(15 150%) 8.5346\n",
      "(16 160%) 8.6071\n",
      "(17 170%) 4.3124\n",
      "(18 180%) 8.5905\n",
      "(19 190%) 8.5432\n",
      "(20 200%) 6.5094\n",
      "(21 210%) 2.4786\n",
      "(22 220%) 8.5457\n",
      "(23 229%) 8.5423\n",
      "(24 240%) 8.5402\n",
      "(25 250%) 4.3090\n",
      "(26 260%) 8.4316\n",
      "(27 270%) 8.5338\n",
      "(28 280%) 3.4414\n",
      "(29 290%) 1.7032\n",
      "(30 300%) 4.2687\n",
      "(31 310%) 4.4129\n",
      "(32 320%) 3.4390\n",
      "(33 330%) 3.3885\n",
      "(34 340%) 6.8198\n",
      "(35 350%) 2.8320\n",
      "(36 360%) 4.2586\n",
      "(37 370%) 2.8885\n",
      "(38 380%) 5.1315\n",
      "(39 390%) 4.3838\n",
      "(40 400%) 4.2994\n",
      "(41 409%) 8.6629\n",
      "(42 420%) 2.0949\n",
      "(43 430%) 3.4752\n",
      "(44 440%) 2.8273\n",
      "(45 450%) 2.8379\n",
      "(46 459%) 4.1699\n",
      "(47 470%) 4.3455\n",
      "(48 480%) 5.9133\n",
      "(49 490%) 5.8873\n",
      "(50 500%) 4.3277\n",
      "(51 509%) 5.0988\n",
      "(52 520%) 5.7807\n",
      "(53 530%) 2.4639\n",
      "(54 540%) 5.0952\n",
      "(55 550%) 5.1763\n",
      "(56 560%) 1.4530\n",
      "(57 570%) 6.5622\n",
      "(58 580%) 4.2463\n",
      "(59 590%) 8.4455\n",
      "(60 600%) 4.1508\n",
      "(61 610%) 4.1595\n",
      "(62 620%) 4.1169\n",
      "(63 630%) 3.4388\n",
      "(64 640%) 4.3231\n",
      "(65 650%) 3.3883\n",
      "(66 660%) 5.0763\n",
      "(67 670%) 5.8843\n",
      "(68 680%) 6.5415\n",
      "(69 690%) 4.3052\n",
      "(70 700%) 1.2098\n",
      "(71 710%) 8.3695\n",
      "(72 720%) 1.7070\n",
      "(73 730%) 2.0410\n",
      "(74 740%) 4.1932\n",
      "(75 750%) 8.6162\n",
      "(76 760%) 5.1310\n",
      "(77 770%) 2.1657\n",
      "(78 780%) 4.1731\n",
      "(79 790%) 4.3657\n",
      "(80 800%) 2.1469\n",
      "(81 810%) 2.1935\n",
      "(82 819%) 2.1769\n",
      "(83 830%) 4.1259\n",
      "(84 840%) 4.2763\n",
      "(85 850%) 2.8883\n",
      "(86 860%) 4.4308\n",
      "(87 869%) 8.2830\n",
      "(88 880%) 6.5106\n",
      "(89 890%) 2.8703\n",
      "(90 900%) 8.3473\n",
      "(91 910%) 2.7719\n",
      "(92 919%) 2.2267\n",
      "(93 930%) 4.2309\n",
      "(94 940%) 5.7089\n",
      "(95 950%) 4.2147\n",
      "(96 960%) 4.4028\n",
      "(97 969%) 2.9601\n",
      "(98 980%) 4.1157\n",
      "(99 990%) 4.4215\n",
      "(100 1000%) 3.4568\n",
      "(101 1010%) 5.2319\n",
      "(102 1019%) 8.5578\n",
      "(103 1030%) 4.3112\n",
      "(104 1040%) 2.7212\n",
      "(105 1050%) 2.1663\n",
      "(106 1060%) 5.8052\n",
      "(107 1070%) 4.4012\n",
      "(108 1080%) 3.5274\n",
      "(109 1090%) 2.9456\n",
      "(110 1100%) 4.1875\n",
      "(111 1110%) 3.5254\n",
      "(112 1120%) 3.5001\n",
      "(113 1130%) 4.3848\n",
      "(114 1140%) 4.2354\n",
      "(115 1150%) 4.2302\n",
      "(116 1160%) 5.7620\n",
      "(117 1170%) 5.8249\n",
      "(118 1180%) 6.2240\n",
      "(119 1190%) 6.4659\n",
      "(120 1200%) 2.7186\n",
      "(121 1210%) 6.5060\n",
      "(122 1220%) 6.3617\n",
      "(123 1230%) 6.5313\n",
      "(124 1240%) 8.4265\n",
      "(125 1250%) 6.3754\n",
      "(126 1260%) 4.2084\n",
      "(127 1270%) 6.5284\n",
      "(128 1280%) 8.4294\n",
      "(129 1290%) 3.4197\n",
      "(130 1300%) 8.3622\n",
      "(131 1310%) 3.5340\n",
      "(132 1320%) 3.7740\n",
      "(133 1330%) 4.3368\n",
      "(134 1340%) 6.1427\n",
      "(135 1350%) 5.1130\n",
      "(136 1360%) 5.0023\n",
      "(137 1370%) 5.1953\n",
      "(138 1380%) 6.5215\n",
      "(139 1390%) 5.2696\n",
      "(140 1400%) 8.4317\n",
      "(141 1410%) 6.4840\n",
      "(142 1420%) 4.4237\n",
      "(143 1430%) 3.5976\n",
      "(144 1440%) 5.1316\n",
      "(145 1450%) 5.0364\n",
      "(146 1460%) 5.0991\n",
      "(147 1470%) 6.3228\n",
      "(148 1480%) 8.4090\n",
      "(149 1490%) 6.0941\n",
      "(150 1500%) 3.5480\n",
      "(151 1510%) 4.3407\n",
      "(152 1520%) 6.4567\n",
      "(153 1530%) 6.5077\n",
      "(154 1540%) 4.8632\n",
      "(155 1550%) 8.3830\n",
      "(156 1560%) 8.2761\n",
      "(157 1570%) 6.2253\n",
      "(158 1580%) 4.4568\n",
      "(159 1590%) 6.5224\n",
      "(160 1600%) 8.3672\n",
      "(161 1610%) 6.3975\n",
      "(162 1620%) 8.3620\n",
      "(163 1630%) 6.4818\n",
      "(164 1639%) 5.2942\n",
      "(165 1650%) 5.0091\n",
      "(166 1660%) 8.0633\n",
      "(167 1670%) 5.1558\n",
      "(168 1680%) 6.4865\n",
      "(169 1689%) 6.5529\n",
      "(170 1700%) 8.0717\n",
      "(171 1710%) 5.1401\n",
      "(172 1720%) 6.5666\n",
      "(173 1730%) 6.0779\n",
      "(174 1739%) 4.0396\n",
      "(175 1750%) 8.1221\n",
      "(176 1760%) 4.9441\n",
      "(177 1770%) 4.2365\n",
      "(178 1780%) 8.1780\n",
      "(179 1789%) 5.8441\n",
      "(180 1800%) 4.0136\n",
      "(181 1810%) 5.2184\n",
      "(182 1820%) 3.1227\n",
      "(183 1830%) 5.1338\n",
      "(184 1839%) 4.4126\n",
      "(185 1850%) 5.1522\n",
      "(186 1860%) 5.8349\n",
      "(187 1870%) 4.2621\n",
      "(188 1880%) 4.2325\n",
      "(189 1889%) 6.5548\n",
      "(190 1900%) 3.1766\n",
      "(191 1910%) 6.6562\n",
      "(192 1920%) 4.3581\n",
      "(193 1930%) 8.1680\n",
      "(194 1939%) 5.9191\n",
      "(195 1950%) 4.9745\n",
      "(196 1960%) 4.4315\n",
      "(197 1970%) 4.2002\n",
      "(198 1980%) 5.5996\n",
      "(199 1989%) 6.3516\n",
      "(200 2000%) 5.9782\n",
      "(201 2010%) 4.2722\n",
      "(202 2020%) 6.3573\n",
      "(203 2030%) 5.2144\n",
      "(204 2039%) 5.0764\n",
      "(205 2050%) 5.0024\n",
      "(206 2060%) 3.4543\n",
      "(207 2070%) 5.2239\n",
      "(208 2080%) 6.3058\n",
      "(209 2090%) 8.3940\n",
      "(210 2100%) 6.3147\n",
      "(211 2110%) 6.4537\n",
      "(212 2120%) 5.8922\n",
      "(213 2130%) 6.4350\n",
      "(214 2140%) 6.4269\n",
      "(215 2150%) 5.2593\n",
      "(216 2160%) 8.1250\n",
      "(217 2170%) 8.1192\n",
      "(218 2180%) 8.3480\n",
      "(219 2190%) 4.1194\n",
      "(220 2200%) 6.3785\n",
      "(221 2210%) 3.6790\n",
      "(222 2220%) 5.2113\n",
      "(223 2230%) 8.1233\n",
      "(224 2240%) 5.2135\n",
      "(225 2250%) 6.4568\n",
      "(226 2260%) 8.2101\n",
      "(227 2270%) 3.7980\n",
      "(228 2280%) 6.4376\n",
      "(229 2290%) 8.1416\n",
      "(230 2300%) 4.6745\n",
      "(231 2310%) 3.9282\n",
      "(232 2320%) 5.2425\n",
      "(233 2330%) 6.3459\n",
      "(234 2340%) 3.8104\n",
      "(235 2350%) 4.3899\n",
      "(236 2360%) 5.1333\n",
      "(237 2370%) 5.1511\n",
      "(238 2380%) 6.2647\n",
      "(239 2390%) 6.5182\n",
      "(240 2400%) 4.8048\n",
      "(241 2410%) 6.9038\n",
      "(242 2420%) 6.5257\n",
      "(243 2430%) 5.9818\n",
      "(244 2440%) 4.0505\n",
      "(245 2450%) 5.0652\n",
      "(246 2460%) 6.7416\n",
      "(247 2470%) 6.4568\n",
      "(248 2480%) 6.4381\n",
      "(249 2490%) 3.2578\n",
      "(250 2500%) 5.3516\n",
      "(251 2510%) 6.4265\n",
      "(252 2520%) 7.8710\n",
      "(253 2530%) 8.2012\n",
      "(254 2540%) 6.6494\n",
      "(255 2550%) 8.1290\n",
      "(256 2560%) 3.1891\n",
      "(257 2570%) 7.8339\n",
      "(258 2580%) 8.1162\n",
      "(259 2590%) 6.4361\n",
      "(260 2600%) 8.1350\n",
      "(261 2610%) 6.8389\n",
      "(262 2620%) 5.1132\n",
      "(263 2630%) 7.9005\n",
      "(264 2640%) 3.6688\n",
      "(265 2650%) 5.6057\n",
      "(266 2660%) 6.1091\n",
      "(267 2670%) 4.4360\n",
      "(268 2680%) 6.1839\n",
      "(269 2690%) 5.0651\n",
      "(270 2700%) 5.0976\n",
      "(271 2710%) 6.3062\n",
      "(272 2720%) 5.0979\n",
      "(273 2730%) 4.7227\n",
      "(274 2740%) 8.0104\n",
      "(275 2750%) 5.3786\n",
      "(276 2760%) 3.9497\n",
      "(277 2770%) 5.3487\n",
      "(278 2780%) 6.3483\n",
      "(279 2790%) 6.4415\n",
      "(280 2800%) 6.7511\n",
      "(281 2810%) 6.8475\n",
      "(282 2820%) 6.8480\n",
      "(283 2830%) 6.9200\n",
      "(284 2840%) 6.5961\n",
      "(285 2850%) 7.4986\n",
      "(286 2860%) 7.5399\n",
      "(287 2870%) 4.3161\n",
      "(288 2880%) 6.2000\n",
      "(289 2890%) 3.4232\n",
      "(290 2900%) 5.9949\n",
      "(291 2910%) 6.4915\n",
      "(292 2920%) 6.6349\n",
      "(293 2930%) 6.5041\n",
      "(294 2940%) 5.4661\n",
      "(295 2950%) 5.2307\n",
      "(296 2960%) 5.5786\n",
      "(297 2970%) 5.8661\n",
      "(298 2980%) 4.1108\n",
      "(299 2990%) 7.6830\n",
      "(300 3000%) 6.1882\n",
      "(301 3010%) 7.3831\n",
      "(302 3020%) 6.3545\n",
      "(303 3030%) 7.8719\n",
      "(304 3040%) 7.0896\n",
      "(305 3050%) 3.8959\n",
      "(306 3060%) 5.1560\n",
      "(307 3070%) 4.6656\n",
      "(308 3080%) 3.4341\n",
      "(309 3090%) 7.3083\n",
      "(310 3100%) 7.2907\n",
      "(311 3110%) 7.0631\n",
      "(312 3120%) 6.0749\n",
      "(313 3130%) 4.3839\n",
      "(314 3140%) 3.7554\n",
      "(315 3150%) 6.9711\n",
      "(316 3160%) 6.5317\n",
      "(317 3170%) 4.2175\n",
      "(318 3180%) 5.3314\n",
      "(319 3190%) 6.3353\n",
      "(320 3200%) 6.5496\n",
      "(321 3210%) 7.3198\n",
      "(322 3220%) 4.1076\n",
      "(323 3229%) 6.1040\n",
      "(324 3240%) 5.0927\n",
      "(325 3250%) 5.8670\n",
      "(326 3260%) 3.4729\n",
      "(327 3270%) 2.3824\n",
      "(328 3279%) 3.6440\n",
      "(329 3290%) 6.0320\n",
      "(330 3300%) 5.7004\n",
      "(331 3310%) 6.4353\n",
      "(332 3320%) 6.4254\n",
      "(333 3329%) 5.0219\n",
      "(334 3340%) 4.8002\n",
      "(335 3350%) 5.3216\n",
      "(336 3360%) 6.6220\n",
      "(337 3370%) 6.9382\n",
      "(338 3379%) 4.3220\n",
      "(339 3390%) 6.5935\n",
      "(340 3400%) 6.3410\n",
      "(341 3410%) 7.1949\n",
      "(342 3420%) 5.7433\n",
      "(343 3429%) 5.9993\n",
      "(344 3440%) 4.9352\n",
      "(345 3450%) 6.2176\n",
      "(346 3460%) 6.0907\n",
      "(347 3470%) 4.3799\n",
      "(348 3479%) 7.3766\n",
      "(349 3490%) 6.3960\n",
      "(350 3500%) 4.2838\n",
      "(351 3510%) 6.2535\n",
      "(352 3520%) 7.0594\n",
      "(353 3529%) 5.0992\n",
      "(354 3540%) 5.1568\n",
      "(355 3550%) 6.3656\n",
      "(356 3560%) 4.9081\n",
      "(357 3570%) 6.3081\n",
      "(358 3579%) 6.0214\n",
      "(359 3590%) 7.2397\n",
      "(360 3600%) 6.7944\n",
      "(361 3610%) 5.8693\n",
      "(362 3620%) 5.4450\n",
      "(363 3629%) 6.3770\n",
      "(364 3640%) 4.6892\n",
      "(365 3650%) 5.0174\n",
      "(366 3660%) 3.6619\n",
      "(367 3670%) 3.1011\n",
      "(368 3679%) 5.0831\n",
      "(369 3690%) 4.9552\n",
      "(370 3700%) 6.0710\n",
      "(371 3710%) 5.2478\n",
      "(372 3720%) 5.0991\n",
      "(373 3729%) 4.8498\n",
      "(374 3740%) 4.7381\n",
      "(375 3750%) 6.4683\n",
      "(376 3760%) 3.5112\n",
      "(377 3770%) 5.7871\n",
      "(378 3779%) 5.4121\n",
      "(379 3790%) 4.7377\n",
      "(380 3800%) 4.4429\n",
      "(381 3810%) 4.8365\n",
      "(382 3820%) 6.5109\n",
      "(383 3829%) 4.7329\n",
      "(384 3840%) 5.0909\n",
      "(385 3850%) 3.4790\n",
      "(386 3860%) 6.2907\n",
      "(387 3870%) 4.6455\n",
      "(388 3879%) 6.1155\n",
      "(389 3890%) 6.6071\n",
      "(390 3900%) 5.5933\n",
      "(391 3910%) 6.0130\n",
      "(392 3920%) 6.5020\n",
      "(393 3929%) 3.8718\n",
      "(394 3940%) 2.8534\n",
      "(395 3950%) 3.4880\n",
      "(396 3960%) 6.5586\n",
      "(397 3970%) 6.4468\n",
      "(398 3979%) 2.8056\n",
      "(399 3990%) 3.6759\n",
      "(400 4000%) 5.3471\n",
      "(401 4010%) 5.2329\n",
      "(402 4020%) 6.6116\n",
      "(403 4029%) 2.7143\n",
      "(404 4040%) 6.1951\n",
      "(405 4050%) 4.6644\n",
      "(406 4060%) 2.1865\n",
      "(407 4070%) 5.0074\n",
      "(408 4079%) 5.5038\n",
      "(409 4090%) 5.2747\n",
      "(410 4100%) 4.6357\n",
      "(411 4110%) 2.0958\n",
      "(412 4120%) 4.1663\n",
      "(413 4130%) 5.7173\n",
      "(414 4140%) 7.1822\n",
      "(415 4150%) 3.0071\n",
      "(416 4160%) 2.6446\n",
      "(417 4170%) 6.0030\n",
      "(418 4180%) 6.3562\n",
      "(419 4190%) 3.4843\n",
      "(420 4200%) 6.4139\n",
      "(421 4210%) 3.9009\n",
      "(422 4220%) 3.0419\n",
      "(423 4230%) 6.0048\n",
      "(424 4240%) 4.5581\n",
      "(425 4250%) 5.0065\n",
      "(426 4260%) 6.2503\n",
      "(427 4270%) 5.3281\n",
      "(428 4280%) 5.9512\n",
      "(429 4290%) 6.5094\n",
      "(430 4300%) 2.9251\n",
      "(431 4310%) 3.9446\n",
      "(432 4320%) 4.4509\n",
      "(433 4330%) 2.9838\n",
      "(434 4340%) 2.2011\n",
      "(435 4350%) 6.5593\n",
      "(436 4360%) 3.8092\n",
      "(437 4370%) 4.9369\n",
      "(438 4380%) 2.0140\n",
      "(439 4390%) 4.8686\n",
      "(440 4400%) 1.9095\n",
      "(441 4410%) 5.1017\n",
      "(442 4420%) 4.0260\n",
      "(443 4430%) 5.8544\n",
      "(444 4440%) 5.3476\n",
      "(445 4450%) 5.7715\n",
      "(446 4460%) 2.8451\n",
      "(447 4470%) 3.9332\n",
      "(448 4480%) 3.7562\n",
      "(449 4490%) 4.3449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450 4500%) 4.2823\n",
      "(451 4510%) 4.2096\n",
      "(452 4520%) 1.7249\n",
      "(453 4530%) 2.7219\n",
      "(454 4540%) 2.7679\n",
      "(455 4550%) 3.4102\n",
      "(456 4560%) 3.6603\n",
      "(457 4570%) 3.2852\n",
      "(458 4580%) 3.2610\n",
      "(459 4590%) 2.2486\n",
      "(460 4600%) 3.4327\n",
      "(461 4610%) 3.4979\n",
      "(462 4620%) 2.5449\n",
      "(463 4630%) 5.7862\n",
      "(464 4640%) 3.6546\n",
      "(465 4650%) 2.5435\n",
      "(466 4660%) 3.5109\n",
      "(467 4670%) 3.5555\n",
      "(468 4680%) 4.3631\n",
      "(469 4690%) 3.7370\n",
      "(470 4700%) 6.0405\n",
      "(471 4710%) 3.9448\n",
      "(472 4720%) 3.3454\n",
      "(473 4730%) 4.2109\n",
      "(474 4740%) 2.1497\n",
      "(475 4750%) 3.3103\n",
      "(476 4760%) 3.8052\n",
      "(477 4770%) 3.5577\n",
      "(478 4780%) 2.1163\n",
      "(479 4790%) 4.3181\n",
      "(480 4800%) 5.9763\n",
      "(481 4810%) 4.4537\n",
      "(482 4820%) 3.6978\n",
      "(483 4830%) 3.7595\n",
      "(484 4840%) 2.1653\n",
      "(485 4850%) 5.8662\n",
      "(486 4860%) 4.4530\n",
      "(487 4870%) 4.3228\n",
      "(488 4880%) 3.3643\n",
      "(489 4890%) 3.4277\n",
      "(490 4900%) 3.2527\n",
      "(491 4910%) 1.9382\n",
      "(492 4920%) 3.1412\n",
      "(493 4930%) 2.6332\n",
      "(494 4940%) 4.3660\n",
      "(495 4950%) 2.2614\n",
      "(496 4960%) 3.3500\n",
      "(497 4970%) 3.6481\n",
      "(498 4980%) 4.1458\n",
      "(499 4990%) 3.3149\n",
      "(500 5000%) 5.2700\n",
      "(501 5010%) 2.3947\n",
      "(502 5020%) 1.9968\n",
      "(503 5030%) 2.7983\n",
      "(504 5040%) 3.3080\n",
      "(505 5050%) 3.7056\n",
      "(506 5060%) 3.6726\n",
      "(507 5070%) 6.7170\n",
      "(508 5080%) 5.9658\n",
      "(509 5090%) 4.6121\n",
      "(510 5100%) 2.3245\n",
      "(511 5110%) 1.5461\n",
      "(512 5120%) 2.9197\n",
      "(513 5130%) 2.3788\n",
      "(514 5140%) 1.8198\n",
      "(515 5150%) 1.4792\n",
      "(516 5160%) 4.3491\n",
      "(517 5170%) 2.9369\n",
      "(518 5180%) 3.3944\n",
      "(519 5190%) 3.2582\n",
      "(520 5200%) 3.1784\n",
      "(521 5210%) 4.1928\n",
      "(522 5220%) 3.0368\n",
      "(523 5230%) 1.8566\n",
      "(524 5240%) 3.7506\n",
      "(525 5250%) 3.4140\n",
      "(526 5260%) 1.8364\n",
      "(527 5270%) 1.7155\n",
      "(528 5280%) 4.3560\n",
      "(529 5290%) 3.3093\n",
      "(530 5300%) 1.3388\n",
      "(531 5310%) 1.6705\n",
      "(532 5320%) 2.5803\n",
      "(533 5330%) 4.4114\n",
      "(534 5340%) 4.4752\n",
      "(535 5350%) 2.7558\n",
      "(536 5360%) 2.6637\n",
      "(537 5370%) 1.6601\n",
      "(538 5380%) 4.6947\n",
      "(539 5390%) 3.8326\n",
      "(540 5400%) 5.9942\n",
      "(541 5410%) 5.3778\n",
      "(542 5420%) 6.6941\n",
      "(543 5430%) 3.7166\n",
      "(544 5440%) 4.1599\n",
      "(545 5450%) 2.7917\n",
      "(546 5460%) 6.9249\n",
      "(547 5470%) 3.3719\n",
      "(548 5480%) 6.1455\n",
      "(549 5490%) 5.9348\n",
      "(550 5500%) 5.5929\n",
      "(551 5510%) 4.4500\n",
      "(552 5520%) 4.4577\n",
      "(553 5530%) 2.9310\n",
      "(554 5540%) 5.7341\n",
      "(555 5550%) 3.6026\n",
      "(556 5560%) 3.4566\n",
      "(557 5570%) 4.3944\n",
      "(558 5580%) 4.6748\n",
      "(559 5590%) 2.6099\n",
      "(560 5600%) 6.2291\n",
      "(561 5610%) 3.2963\n",
      "(562 5620%) 6.0024\n",
      "(563 5630%) 5.8656\n",
      "(564 5640%) 3.3235\n",
      "(565 5650%) 1.2761\n",
      "(566 5660%) 4.3257\n",
      "(567 5670%) 4.2262\n",
      "(568 5680%) 2.9110\n",
      "(569 5690%) 3.0062\n",
      "(570 5700%) 4.0826\n",
      "(571 5710%) 3.9870\n",
      "(572 5720%) 1.5832\n",
      "(573 5730%) 6.0574\n",
      "(574 5740%) 2.0439\n",
      "(575 5750%) 1.5190\n",
      "(576 5760%) 3.4667\n",
      "(577 5770%) 3.6539\n",
      "(578 5780%) 4.9218\n",
      "(579 5790%) 2.8538\n",
      "(580 5800%) 3.4144\n",
      "(581 5810%) 2.0637\n",
      "(582 5820%) 1.9121\n",
      "(583 5830%) 5.0358\n",
      "(584 5840%) 5.6936\n",
      "(585 5850%) 4.0088\n",
      "(586 5860%) 6.2877\n",
      "(587 5870%) 5.1802\n",
      "(588 5880%) 3.3967\n",
      "(589 5890%) 2.5095\n",
      "(590 5900%) 2.3228\n",
      "(591 5910%) 3.8605\n",
      "(592 5920%) 3.5057\n",
      "(593 5930%) 3.0245\n",
      "(594 5940%) 2.7439\n",
      "(595 5950%) 3.1869\n",
      "(596 5960%) 4.1283\n",
      "(597 5970%) 2.1445\n",
      "(598 5980%) 5.0661\n",
      "(599 5990%) 5.8178\n",
      "(600 6000%) 2.6512\n",
      "(601 6010%) 5.5870\n",
      "(602 6020%) 5.4972\n",
      "(603 6030%) 3.2058\n",
      "(604 6040%) 4.0529\n",
      "(605 6050%) 3.4056\n",
      "(606 6060%) 2.3440\n",
      "(607 6070%) 2.1010\n",
      "(608 6080%) 6.4184\n",
      "(609 6090%) 2.0397\n",
      "(610 6100%) 6.5287\n",
      "(611 6110%) 6.0522\n",
      "(612 6120%) 3.8455\n",
      "(613 6130%) 2.1335\n",
      "(614 6140%) 4.4057\n",
      "(615 6150%) 4.6545\n",
      "(616 6160%) 2.3239\n",
      "(617 6170%) 2.6660\n",
      "(618 6180%) 1.3482\n",
      "(619 6190%) 4.5498\n",
      "(620 6200%) 2.8031\n",
      "(621 6210%) 1.5353\n",
      "(622 6220%) 3.5309\n",
      "(623 6230%) 2.8831\n",
      "(624 6240%) 5.2327\n",
      "(625 6250%) 3.3546\n",
      "(626 6260%) 3.4963\n",
      "(627 6270%) 2.9623\n",
      "(628 6280%) 1.8027\n",
      "(629 6290%) 4.5776\n",
      "(630 6300%) 3.3769\n",
      "(631 6310%) 2.7984\n",
      "(632 6320%) 3.7383\n",
      "(633 6330%) 3.9339\n",
      "(634 6340%) 4.5539\n",
      "(635 6350%) 4.6458\n",
      "(636 6360%) 1.7078\n",
      "(637 6370%) 2.3490\n",
      "(638 6380%) 1.6269\n",
      "(639 6390%) 5.9640\n",
      "(640 6400%) 2.0941\n",
      "(641 6409%) 1.1418\n",
      "(642 6420%) 1.9111\n",
      "(643 6430%) 4.2471\n",
      "(644 6440%) 2.3893\n",
      "(645 6450%) 3.0126\n",
      "(646 6459%) 5.1205\n",
      "(647 6470%) 2.4247\n",
      "(648 6480%) 3.0863\n",
      "(649 6490%) 3.7470\n",
      "(650 6500%) 2.3902\n",
      "(651 6509%) 3.7136\n",
      "(652 6520%) 3.5697\n",
      "(653 6530%) 3.5610\n",
      "(654 6540%) 4.5072\n",
      "(655 6550%) 3.6469\n",
      "(656 6559%) 2.6561\n",
      "(657 6570%) 5.9210\n",
      "(658 6580%) 2.8424\n",
      "(659 6590%) 5.3689\n",
      "(660 6600%) 6.1156\n",
      "(661 6609%) 1.5631\n",
      "(662 6620%) 3.1790\n",
      "(663 6630%) 4.1169\n",
      "(664 6640%) 2.9237\n",
      "(665 6650%) 3.0926\n",
      "(666 6659%) 4.9236\n",
      "(667 6670%) 2.3388\n",
      "(668 6680%) 3.3673\n",
      "(669 6690%) 1.1450\n",
      "(670 6700%) 1.3584\n",
      "(671 6709%) 3.4538\n",
      "(672 6720%) 1.2918\n",
      "(673 6730%) 5.1245\n",
      "(674 6740%) 3.5989\n",
      "(675 6750%) 3.7218\n",
      "(676 6759%) 2.6705\n",
      "(677 6770%) 4.5040\n",
      "(678 6780%) 4.3681\n",
      "(679 6790%) 2.6142\n",
      "(680 6800%) 3.3954\n",
      "(681 6809%) 4.6149\n",
      "(682 6820%) 3.2422\n",
      "(683 6830%) 1.5288\n",
      "(684 6840%) 2.6833\n",
      "(685 6850%) 3.3269\n",
      "(686 6859%) 2.7263\n",
      "(687 6870%) 4.1383\n",
      "(688 6880%) 1.6649\n",
      "(689 6890%) 3.7351\n",
      "(690 6900%) 3.2197\n",
      "(691 6909%) 5.0049\n",
      "(692 6920%) 1.3898\n",
      "(693 6930%) 4.1449\n",
      "(694 6940%) 3.3108\n",
      "(695 6950%) 1.3077\n",
      "(696 6959%) 3.2575\n",
      "(697 6970%) 3.6432\n",
      "(698 6980%) 3.2537\n",
      "(699 6990%) 3.9093\n",
      "(700 7000%) 4.9989\n",
      "(701 7009%) 5.9894\n",
      "(702 7020%) 4.4510\n",
      "(703 7030%) 6.8707\n",
      "(704 7040%) 2.9306\n",
      "(705 7050%) 2.2311\n",
      "(706 7059%) 2.0540\n",
      "(707 7070%) 6.3356\n",
      "(708 7080%) 4.7021\n",
      "(709 7090%) 2.5897\n",
      "(710 7100%) 3.4615\n",
      "(711 7109%) 3.5060\n",
      "(712 7120%) 4.0662\n",
      "(713 7130%) 4.3213\n",
      "(714 7140%) 2.1929\n",
      "(715 7150%) 1.4013\n",
      "(716 7159%) 3.5224\n",
      "(717 7170%) 1.6081\n",
      "(718 7180%) 4.7256\n",
      "(719 7190%) 1.8229\n",
      "(720 7200%) 3.1041\n",
      "(721 7209%) 3.8908\n",
      "(722 7220%) 4.2357\n",
      "(723 7230%) 2.6611\n",
      "(724 7240%) 4.5545\n",
      "(725 7250%) 3.1328\n",
      "(726 7259%) 3.4135\n",
      "(727 7270%) 3.6771\n",
      "(728 7280%) 1.9004\n",
      "(729 7290%) 2.8050\n",
      "(730 7300%) 1.7721\n",
      "(731 7309%) 2.0645\n",
      "(732 7320%) 4.6889\n",
      "(733 7330%) 3.3958\n",
      "(734 7340%) 2.2474\n",
      "(735 7350%) 2.0457\n",
      "(736 7359%) 4.2297\n",
      "(737 7370%) 1.0957\n",
      "(738 7380%) 6.6476\n",
      "(739 7390%) 2.4881\n",
      "(740 7400%) 0.9926\n",
      "(741 7409%) 6.2946\n",
      "(742 7420%) 1.7856\n",
      "(743 7430%) 2.4651\n",
      "(744 7440%) 4.5437\n",
      "(745 7450%) 4.7639\n",
      "(746 7459%) 3.0339\n",
      "(747 7470%) 4.3908\n",
      "(748 7480%) 4.7378\n",
      "(749 7490%) 2.2328\n",
      "(750 7500%) 2.5486\n",
      "(751 7509%) 3.6595\n",
      "(752 7520%) 1.9306\n",
      "(753 7530%) 3.6474\n",
      "(754 7540%) 4.2482\n",
      "(755 7550%) 3.5587\n",
      "(756 7559%) 3.5662\n",
      "(757 7570%) 4.2908\n",
      "(758 7580%) 3.4834\n",
      "(759 7590%) 3.2771\n",
      "(760 7600%) 2.0082\n",
      "(761 7609%) 3.8096\n",
      "(762 7620%) 2.3563\n",
      "(763 7630%) 2.5963\n",
      "(764 7640%) 3.1005\n",
      "(765 7650%) 6.7091\n",
      "(766 7659%) 3.7617\n",
      "(767 7670%) 1.6893\n",
      "(768 7680%) 6.3910\n",
      "(769 7690%) 3.0107\n",
      "(770 7700%) 5.7306\n",
      "(771 7709%) 2.5498\n",
      "(772 7720%) 4.3753\n",
      "(773 7730%) 2.1407\n",
      "(774 7740%) 2.3632\n",
      "(775 7750%) 2.9388\n",
      "(776 7759%) 1.3024\n",
      "(777 7770%) 2.9674\n",
      "(778 7780%) 5.7261\n",
      "(779 7790%) 3.8086\n",
      "(780 7800%) 6.4130\n",
      "(781 7809%) 4.4628\n",
      "(782 7820%) 4.1807\n",
      "(783 7830%) 2.6404\n",
      "(784 7840%) 3.0836\n",
      "(785 7850%) 1.3669\n",
      "(786 7859%) 6.4617\n",
      "(787 7870%) 5.1005\n",
      "(788 7880%) 4.5454\n",
      "(789 7890%) 7.2378\n",
      "(790 7900%) 3.8071\n",
      "(791 7909%) 1.6439\n",
      "(792 7920%) 3.2124\n",
      "(793 7930%) 4.3956\n",
      "(794 7940%) 2.0869\n",
      "(795 7950%) 1.8493\n",
      "(796 7959%) 2.2174\n",
      "(797 7970%) 4.6855\n",
      "(798 7980%) 4.1742\n",
      "(799 7990%) 4.4382\n",
      "(800 8000%) 4.1646\n",
      "(801 8009%) 5.3012\n",
      "(802 8020%) 3.2667\n",
      "(803 8030%) 1.2121\n",
      "(804 8040%) 1.7095\n",
      "(805 8050%) 4.5438\n",
      "(806 8059%) 1.5080\n",
      "(807 8070%) 7.0666\n",
      "(808 8080%) 1.5747\n",
      "(809 8090%) 2.0863\n",
      "(810 8100%) 4.8753\n",
      "(811 8109%) 1.0510\n",
      "(812 8120%) 4.5738\n",
      "(813 8130%) 2.7660\n",
      "(814 8140%) 2.6466\n",
      "(815 8150%) 5.4654\n",
      "(816 8159%) 2.2867\n",
      "(817 8170%) 3.5675\n",
      "(818 8180%) 1.6833\n",
      "(819 8190%) 4.1761\n",
      "(820 8200%) 6.2850\n",
      "(821 8210%) 2.4895\n",
      "(822 8220%) 0.9976\n",
      "(823 8230%) 3.9422\n",
      "(824 8240%) 6.1637\n",
      "(825 8250%) 3.0058\n",
      "(826 8260%) 2.6740\n",
      "(827 8270%) 3.7537\n",
      "(828 8280%) 2.1793\n",
      "(829 8290%) 3.1472\n",
      "(830 8300%) 1.7354\n",
      "(831 8310%) 2.6292\n",
      "(832 8320%) 4.2414\n",
      "(833 8330%) 3.1423\n",
      "(834 8340%) 2.4060\n",
      "(835 8350%) 4.2214\n",
      "(836 8360%) 3.4668\n",
      "(837 8370%) 2.3908\n",
      "(838 8380%) 3.4341\n",
      "(839 8390%) 0.9686\n",
      "(840 8400%) 1.2245\n",
      "(841 8410%) 1.9406\n",
      "(842 8420%) 4.8700\n",
      "(843 8430%) 5.2205\n",
      "(844 8440%) 4.9192\n",
      "(845 8450%) 5.4642\n",
      "(846 8460%) 7.5681\n",
      "(847 8470%) 3.9707\n",
      "(848 8480%) 2.4016\n",
      "(849 8490%) 4.3612\n",
      "(850 8500%) 3.0600\n",
      "(851 8510%) 3.9629\n",
      "(852 8520%) 2.4238\n",
      "(853 8530%) 3.2403\n",
      "(854 8540%) 4.0487\n",
      "(855 8550%) 3.3347\n",
      "(856 8560%) 1.4723\n",
      "(857 8570%) 3.6053\n",
      "(858 8580%) 2.2544\n",
      "(859 8590%) 2.6866\n",
      "(860 8600%) 1.8088\n",
      "(861 8610%) 2.3632\n",
      "(862 8620%) 3.3987\n",
      "(863 8630%) 4.5678\n",
      "(864 8640%) 2.3389\n",
      "(865 8650%) 1.9491\n",
      "(866 8660%) 2.3039\n",
      "(867 8670%) 2.4737\n",
      "(868 8680%) 2.0774\n",
      "(869 8690%) 1.8916\n",
      "(870 8700%) 1.6281\n",
      "(871 8710%) 3.8585\n",
      "(872 8720%) 4.4706\n",
      "(873 8730%) 1.7090\n",
      "(874 8740%) 3.4638\n",
      "(875 8750%) 1.3214\n",
      "(876 8760%) 4.5861\n",
      "(877 8770%) 3.5631\n",
      "(878 8780%) 5.0403\n",
      "(879 8790%) 4.6378\n",
      "(880 8800%) 3.4911\n",
      "(881 8810%) 5.7619\n",
      "(882 8820%) 4.2246\n",
      "(883 8830%) 3.4269\n",
      "(884 8840%) 0.8249\n",
      "(885 8850%) 4.2767\n",
      "(886 8860%) 1.1726\n",
      "(887 8870%) 3.1638\n",
      "(888 8880%) 3.6090\n",
      "(889 8890%) 5.0493\n",
      "(890 8900%) 5.4001\n",
      "(891 8910%) 3.2245\n",
      "(892 8920%) 3.0006\n",
      "(893 8930%) 3.6417\n",
      "(894 8940%) 4.0822\n",
      "(895 8950%) 6.3095\n",
      "(896 8960%) 1.8673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(897 8970%) 3.5051\n",
      "(898 8980%) 0.8193\n",
      "(899 8990%) 3.3247\n",
      "(900 9000%) 5.4609\n",
      "(901 9010%) 1.9391\n",
      "(902 9020%) 3.7431\n",
      "(903 9030%) 6.4488\n",
      "(904 9040%) 6.5875\n",
      "(905 9050%) 5.7826\n",
      "(906 9060%) 1.0259\n",
      "(907 9070%) 6.9079\n",
      "(908 9080%) 0.9773\n",
      "(909 9090%) 5.1056\n",
      "(910 9100%) 5.3012\n",
      "(911 9110%) 5.4456\n",
      "(912 9120%) 6.1552\n",
      "(913 9130%) 5.0608\n",
      "(914 9140%) 2.3290\n",
      "(915 9150%) 2.6340\n",
      "(916 9160%) 1.8932\n",
      "(917 9170%) 3.2356\n",
      "(918 9180%) 1.7427\n",
      "(919 9190%) 0.8359\n",
      "(920 9200%) 3.3007\n",
      "(921 9210%) 4.9826\n",
      "(922 9220%) 3.1634\n",
      "(923 9230%) 6.1926\n",
      "(924 9240%) 5.0756\n",
      "(925 9250%) 1.7487\n",
      "(926 9260%) 4.9537\n",
      "(927 9270%) 4.8321\n",
      "(928 9280%) 3.7545\n",
      "(929 9290%) 4.3915\n",
      "(930 9300%) 6.2651\n",
      "(931 9310%) 3.1870\n",
      "(932 9320%) 3.7547\n",
      "(933 9330%) 2.3100\n",
      "(934 9340%) 2.5958\n",
      "(935 9350%) 4.5649\n",
      "(936 9360%) 2.9295\n",
      "(937 9370%) 6.3052\n",
      "(938 9380%) 2.1842\n",
      "(939 9390%) 3.8323\n",
      "(940 9400%) 4.0369\n",
      "(941 9410%) 2.2654\n",
      "(942 9420%) 2.2144\n",
      "(943 9430%) 2.4930\n",
      "(944 9440%) 1.9472\n",
      "(945 9450%) 3.5273\n",
      "(946 9460%) 3.9638\n",
      "(947 9470%) 6.0239\n",
      "(948 9480%) 2.5853\n",
      "(949 9490%) 2.5372\n",
      "(950 9500%) 4.6645\n",
      "(951 9510%) 5.9239\n",
      "(952 9520%) 1.1515\n",
      "(953 9530%) 1.4621\n",
      "(954 9540%) 6.4424\n",
      "(955 9550%) 6.2676\n",
      "(956 9560%) 6.2371\n",
      "(957 9570%) 2.5672\n",
      "(958 9580%) 5.8041\n",
      "(959 9590%) 2.9455\n",
      "(960 9600%) 6.5210\n",
      "(961 9610%) 1.5356\n",
      "(962 9620%) 3.4179\n",
      "(963 9630%) 4.3820\n",
      "(964 9640%) 5.3677\n",
      "(965 9650%) 3.4890\n",
      "(966 9660%) 5.8652\n",
      "(967 9670%) 5.3939\n",
      "(968 9680%) 2.4690\n",
      "(969 9690%) 5.8507\n",
      "(970 9700%) 2.7676\n",
      "(971 9710%) 1.2614\n",
      "(972 9720%) 2.6346\n",
      "(973 9730%) 3.9697\n",
      "(974 9740%) 4.3141\n",
      "(975 9750%) 3.9111\n",
      "(976 9760%) 2.4267\n",
      "(977 9770%) 4.7433\n",
      "(978 9780%) 5.8916\n",
      "(979 9790%) 2.0864\n",
      "(980 9800%) 2.8780\n",
      "(981 9810%) 1.8843\n",
      "(982 9820%) 2.6268\n",
      "(983 9830%) 3.2485\n",
      "(984 9840%) 3.9103\n",
      "(985 9850%) 5.5140\n",
      "(986 9860%) 3.3372\n",
      "(987 9870%) 5.4651\n",
      "(988 9880%) 1.1872\n",
      "(989 9890%) 4.7723\n",
      "(990 9900%) 3.1712\n",
      "(991 9910%) 2.0423\n",
      "(992 9920%) 4.4267\n",
      "(993 9930%) 3.6510\n",
      "(994 9940%) 5.7826\n",
      "(995 9950%) 4.4915\n",
      "(996 9960%) 2.3348\n",
      "(997 9970%) 3.6339\n",
      "(998 9980%) 2.1627\n",
      "(999 9990%) 6.0879\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(len(input_vocab2index)+2, 30)\n",
    "attn_decoder1 = AttnDecoderRNN(30, len(output_vocab2index)+2, dropout_p=0.1)\n",
    "\n",
    "#attn_decoder1 = DecoderRNN(len(output_vocab2index)+2, 30)\n",
    "\n",
    "encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=0.01)\n",
    "decoder_optimizer = torch.optim.SGD(attn_decoder1.parameters(), lr=0.01)\n",
    "training_pairs = np.random.randint(0, len(input_texts), size=10000)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "print_loss_total = 0\n",
    "for i in range(1000):\n",
    "    input_tensor, target_tensor = tensorsFromSent(input_texts[training_pairs[i]], target_texts[training_pairs[i]])\n",
    "\n",
    "    loss = train(input_tensor, target_tensor, encoder,\n",
    "               attn_decoder1, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    print_loss_total += loss\n",
    "    \n",
    "    print_loss_avg = print_loss_total / 1\n",
    "    print_loss_total = 0\n",
    "    print('(%d %d%%) %.4f' % (i, i / 10 * 100, print_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d32a106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
